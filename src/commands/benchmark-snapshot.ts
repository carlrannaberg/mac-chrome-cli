/**\n * @fileoverview CLI command for running snapshot performance benchmarks\n * \n * This command provides tools to validate the O(n) complexity improvements\n * in the DOM traversal algorithms and ensure performance targets are met.\n * \n * @author mac-chrome-cli\n * @version 1.0.0\n */\n\nimport { Command } from 'commander';\nimport { \n  runBenchmarkSuite, \n  runSnapshotBenchmark, \n  analyzeBenchmarkResults,\n  exportBenchmarkResults,\n  type BenchmarkResult\n} from '../performance/SnapshotBenchmark.js';\nimport { formatOutput } from '../lib/util.js';\n\n/**\n * Options for the benchmark command\n */\ninterface BenchmarkOptions {\n  /** Run full benchmark suite */\n  suite?: boolean;\n  /** Number of DOM nodes for single test */\n  nodes?: number;\n  /** Operation type to benchmark */\n  operation?: 'outline' | 'dom-lite';\n  /** Only capture visible elements */\n  visibleOnly?: boolean;\n  /** Maximum depth for DOM-lite mode */\n  maxDepth?: number;\n  /** Export results to file */\n  export?: string;\n  /** Output format */\n  format?: 'json' | 'table';\n  /** Verbose output */\n  verbose?: boolean;\n}\n\n/**\n * Formats benchmark results for display\n */\nfunction formatBenchmarkResults(results: BenchmarkResult[], options: BenchmarkOptions): string {\n  if (options.format === 'json') {\n    return JSON.stringify(results, null, 2);\n  }\n  \n  // Table format\n  let output = '\\nüìä Snapshot Performance Benchmark Results\\n';\n  output += '=' .repeat(60) + '\\n\\n';\n  \n  // Summary table\n  const passedCount = results.filter(r => r.passedTargets).length;\n  const avgDuration = results.reduce((sum, r) => sum + r.durationMs, 0) / results.length;\n  const avgTimePerNode = results.reduce((sum, r) => sum + r.timePerNodeUs, 0) / results.length;\n  \n  output += `üìà Summary:\\n`;\n  output += `   Tests Run: ${results.length}\\n`;\n  output += `   Passed: ${passedCount}/${results.length} (${(passedCount/results.length*100).toFixed(1)}%)\\n`;\n  output += `   Avg Duration: ${avgDuration.toFixed(2)}ms\\n`;\n  output += `   Avg Time/Node: ${avgTimePerNode.toFixed(2)}Œºs\\n\\n`;\n  \n  // Detailed results table\n  output += '| Test Name'.padEnd(25) + '| Nodes'.padEnd(8) + '| Duration'.padEnd(12) + '| Memory'.padEnd(10) + '| Complexity'.padEnd(12) + '| Status |\\n';\n  output += '|' + '-'.repeat(24) + '|' + '-'.repeat(7) + '|' + '-'.repeat(11) + '|' + '-'.repeat(9) + '|' + '-'.repeat(11) + '|' + '-'.repeat(8) + '|\\n';\n  \n  for (const result of results) {\n    const name = result.testName.length > 23 ? result.testName.substring(0, 20) + '...' : result.testName;\n    const nodes = result.nodeCount.toString();\n    const duration = `${result.durationMs.toFixed(1)}ms`;\n    const memory = `${result.memoryPeakMB.toFixed(1)}MB`;\n    const complexity = result.complexityIndicator;\n    const status = result.passedTargets ? '‚úÖ PASS' : '‚ùå FAIL';\n    \n    output += `| ${name.padEnd(23)} | ${nodes.padEnd(6)} | ${duration.padEnd(10)} | ${memory.padEnd(8)} | ${complexity.padEnd(10)} | ${status} |\\n`;\n  }\n  \n  output += '\\n';\n  \n  // Performance analysis\n  const analysis = analyzeBenchmarkResults(results);\n  \n  output += 'üîç Performance Analysis:\\n';\n  output += `   Overall Complexity: ${analysis.overallComplexity}\\n`;\n  output += `   Linearity Score: ${analysis.detailedAnalysis.linearityScore.toFixed(1)}/100\\n`;\n  output += `   Scaling Factor: ${analysis.detailedAnalysis.scalingFactor.toFixed(2)}x\\n`;\n  output += `   Memory Efficiency: ${analysis.detailedAnalysis.memoryEfficiency.toFixed(2)}KB/node\\n\\n`;\n  \n  // Recommendations\n  output += 'üí° Recommendations:\\n';\n  for (const recommendation of analysis.recommendations) {\n    output += `   ${recommendation}\\n`;\n  }\n  \n  if (options.verbose) {\n    output += '\\nüî¨ Verbose Details:\\n';\n    for (const result of results) {\n      output += `\\n   ${result.testName}:\\n`;\n      output += `     Time per node: ${result.timePerNodeUs.toFixed(2)}Œºs\\n`;\n      output += `     Memory per node: ${result.memoryPerNodeKB.toFixed(2)}KB\\n`;\n      \n      if (result.metadata?.snapshotMetadata?.performance) {\n        const perf = result.metadata.snapshotMetadata.performance;\n        output += `     Algorithm: ${perf.algorithm}\\n`;\n        output += `     Processing time: ${perf.processingMs}ms\\n`;\n        if (perf.traversalMs) {\n          output += `     Traversal time: ${perf.traversalMs}ms\\n`;\n        }\n        if (perf.algorithmsUsed) {\n          output += `     Optimizations: ${perf.algorithmsUsed.join(', ')}\\n`;\n        }\n      }\n    }\n  }\n  \n  return output;\n}\n\n/**\n * Runs a single benchmark test\n */\nasync function runSingleBenchmark(options: BenchmarkOptions): Promise<BenchmarkResult> {\n  const nodeCount = options.nodes || 1000;\n  const operation = options.operation || 'outline';\n  const testName = `Custom ${operation} test`;\n  \n  const benchmarkOptions: any = {};\n  if (options.visibleOnly) benchmarkOptions.visibleOnly = true;\n  if (options.maxDepth) benchmarkOptions.maxDepth = options.maxDepth;\n  \n  console.log(`Running single benchmark: ${operation} with ${nodeCount} nodes...`);\n  \n  return runSnapshotBenchmark(testName, nodeCount, operation, benchmarkOptions);\n}\n\n/**\n * Main benchmark command implementation\n */\nexport async function runBenchmarkCommand(options: BenchmarkOptions): Promise<void> {\n  try {\n    let results: BenchmarkResult[];\n    \n    if (options.suite) {\n      console.log('Running comprehensive benchmark suite...');\n      results = await runBenchmarkSuite();\n    } else {\n      const singleResult = await runSingleBenchmark(options);\n      results = [singleResult];\n    }\n    \n    // Format and display results\n    const formattedOutput = formatBenchmarkResults(results, options);\n    console.log(formattedOutput);\n    \n    // Export results if requested\n    if (options.export) {\n      const analysis = analyzeBenchmarkResults(results);\n      const exportData = exportBenchmarkResults(results, analysis);\n      \n      const fs = await import('fs/promises');\n      await fs.writeFile(options.export, exportData, 'utf8');\n      console.log(`\\nüìÅ Results exported to: ${options.export}`);\n    }\n    \n    // Exit with appropriate code\n    const allPassed = results.every(r => r.passedTargets);\n    if (!allPassed) {\n      console.log('\\n‚ö†Ô∏è  Some performance targets were not met.');\n      process.exit(1);\n    } else {\n      console.log('\\n‚úÖ All performance targets met!');\n    }\n  } catch (error) {\n    console.error('‚ùå Benchmark failed:', error instanceof Error ? error.message : 'Unknown error');\n    process.exit(1);\n  }\n}\n\n/**\n * Creates the CLI command for snapshot benchmarking\n */\nexport function createBenchmarkCommand(): Command {\n  const command = new Command('benchmark-snapshot')\n    .description('Run performance benchmarks for snapshot operations')\n    .option('-s, --suite', 'Run full benchmark suite across different DOM sizes')\n    .option('-n, --nodes <number>', 'Number of DOM nodes for single test', parseInt)\n    .option('-o, --operation <type>', 'Operation type to benchmark', 'outline')\n    .option('--visible-only', 'Only capture visible elements')\n    .option('--max-depth <number>', 'Maximum depth for DOM-lite mode', parseInt)\n    .option('-e, --export <file>', 'Export results to JSON file')\n    .option('-f, --format <type>', 'Output format (json|table)', 'table')\n    .option('-v, --verbose', 'Verbose output with detailed metrics')\n    .action(async (options: BenchmarkOptions) => {\n      await runBenchmarkCommand(options);\n    });\n  \n  // Add examples to help\n  command.addHelpText('after', `\nExamples:\n  # Run full benchmark suite\n  mac-chrome-cli benchmark-snapshot --suite\n  \n  # Test specific configuration\n  mac-chrome-cli benchmark-snapshot --nodes 2000 --operation dom-lite --max-depth 8\n  \n  # Export results for analysis\n  mac-chrome-cli benchmark-snapshot --suite --export benchmark-results.json\n  \n  # Verbose output with optimization details\n  mac-chrome-cli benchmark-snapshot --suite --verbose\n`);\n  \n  return command;\n}\n\n// Export for use in main CLI\nexport default createBenchmarkCommand;"